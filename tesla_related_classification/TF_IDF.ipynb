{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jinho/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/jinho/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()  # convert to lowercase\n",
    "    text = ''.join([c for c in text if c not in string.punctuation])  # remove punctuation\n",
    "    tokens = word_tokenize(text)  # tokenize\n",
    "    tokens = [token for token in tokens if token not in stop_words]  # remove stopwords\n",
    "    tokens = [stemmer.stem(token) for token in tokens]  # stem\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41604, 2266)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('../news_data/Tesla/tesla_news_raw.csv')\n",
    "\n",
    "# Define a TF-IDF Vectorizer Object. Remove all english stop words\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Replace NaN with an empty string\n",
    "df = df.fillna('')\n",
    "\n",
    "# Perform the necessary preprocessing steps for the 'description' column\n",
    "df['description'] = df['description'].apply(lambda x: preprocess(x))\n",
    "\n",
    "# Construct the required TF-IDF matrix by applying the fit_transform method on the 'description' column\n",
    "tfidf_matrix = tfidf.fit_transform(df['description'])\n",
    "\n",
    "#Output the shape of tfidf_matrix\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "# To get the feature names (here, the words)\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# Assuming you want top 10 keywords per document\n",
    "top_n = 10\n",
    "keywords_list = []\n",
    "\n",
    "# iterate through each document\n",
    "for doc in range(tfidf_matrix.shape[0]):\n",
    "    feature_index = tfidf_matrix[doc,:].nonzero()[1]\n",
    "    tfidf_scores = zip(feature_index, [tfidf_matrix[doc, x] for x in feature_index])\n",
    "    words_scores = [(feature_names[i], s) for (i, s) in tfidf_scores]\n",
    "    sorted_words_scores = sorted(words_scores, key=lambda x: x[1], reverse=True)\n",
    "    keywords = sorted_words_scores[:top_n]\n",
    "    keywords_list.append(keywords)\n",
    "\n",
    "df['keywords'] = keywords_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is' 'model_3' 'selling' 'well']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Let's say we have some texts\n",
    "texts = [\"model_3 is selling well\"]\n",
    "\n",
    "# Initialize the vectorizer and fit it with data\n",
    "vectorizer = TfidfVectorizer().fit(texts)\n",
    "\n",
    "# Now we can access the feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# This will output: ['document', 'first', 'is', 'second', 'the', 'this']\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_related(sentence):\n",
    "    sentence = preprocess(sentence)\n",
    "    sentence_tokens = word_tokenize(sentence)\n",
    "\n",
    "    for word in sentence_tokens:\n",
    "        if word in feature_names:\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: well, TF-IDF: 99.50341683926783\n",
      "Word: model_3, TF-IDF: 9.910367613139847\n",
      "Word: is, TF-IDF: 2.9028845001182306\n",
      "Word: selling, TF-IDF: 2.0128372239843264\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Let's get the top 10 keywords\n",
    "top_n = 10\n",
    "\n",
    "# Sum the TF-IDF scores for each term through all documents\n",
    "sum_tfidf = np.sum(tfidf_matrix, axis=0)\n",
    "\n",
    "# Get tuples of (score, word) and sort by the score in descending order\n",
    "words_scores = [(score, word) for word, score in zip(feature_names, np.ravel(sum_tfidf))]\n",
    "sorted_words_scores = sorted(words_scores, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Get the top n words\n",
    "top_keywords = sorted_words_scores[:top_n]\n",
    "\n",
    "for score, word in top_keywords:\n",
    "    print(f'Word: {word}, TF-IDF: {score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['indian', 'soon', 'possibl', 'india', 'modi', 'sector', 'mobil', 'mr', 'commerci', 'musk'], ['india', 'modi', 'popul', 'invest', 'meet', 'promot', 'indian', 'offic', 'chief', 'narendra'], ['nomenclatur', 'nicknam', 'nag', 'select', 'configur', 'research', 'fsd', 'secret', 'wheel', 'intern'], ['kia', 'ev5', '2027', 'autocar', 'hilbert', 'europ', 'gone', '15', 'david', 'introduc'], ['cramer', 'believ', 'mutual', 'doesnt', 'bet', 'ford', 'exclus', 'think', 'buy', 'valu'], ['rivian', 'inch', 'join', 'network', 'closer', 'becom', 'charg', 'follow', 'say', 'maker'], ['handsfre', 'hacker', 'discov', 'greentheonli', 'aptli', 'supersecret', 'onlin', 'mode', 'elon', 'hidden'], ['india', 'modi', 'starlink', 'bring', 'distant', 'say', 'musk', 'look', 'humanli', 'someth'], ['microsoft', 'club', 'trilliondollar', 'alphabet', 'aramco', 'saudi', 'trillion', '123', 'amazon', 'mileston'], ['india', 'invest', 'look', 'humanli', 'soon', 'possibl', 'chief', 'modi', 'elon', 'musk']]\n"
     ]
    }
   ],
   "source": [
    "print(keywords_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
